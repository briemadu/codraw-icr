{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/blasota/anaconda3/envs/codraw_pl/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from content import SELECTED_TOKENS\n",
    "from icr.dataloader import CodrawData\n",
    "from icr.config import args, LABEL_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets for the drawer and the teller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS = ('train', 'val', 'test')\n",
    "ICR_LABEL = LABEL_MAP['icr']\n",
    "\n",
    "MAX_ITER = 1000\n",
    "MIN_FREQ = 2\n",
    "UNK_TOKEN = '<UNK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      " Loaded train set with:\n",
      "   62067 datapoints\n",
      "   7989 dialogues\n",
      "   7016 (11.30%) clarifications\n",
      "   55051 (88.70%) other \n",
      "------------------------------\n",
      "------------------------------\n",
      " Loaded val set with:\n",
      "   7714 datapoints\n",
      "   1002 dialogues\n",
      "   920 (11.93%) clarifications\n",
      "   6794 (88.07%) other \n",
      "------------------------------\n",
      "------------------------------\n",
      " Loaded test set with:\n",
      "   7721 datapoints\n",
      "   1002 dialogues\n",
      "   871 (11.28%) clarifications\n",
      "   6850 (88.72%) other \n",
      "------------------------------\n",
      "------------------------------\n",
      " Loaded train set with:\n",
      "   62067 datapoints\n",
      "   7989 dialogues\n",
      "   7016 (11.30%) clarifications\n",
      "   55051 (88.70%) other \n",
      "------------------------------\n",
      "------------------------------\n",
      " Loaded val set with:\n",
      "   7714 datapoints\n",
      "   1002 dialogues\n",
      "   920 (11.93%) clarifications\n",
      "   6794 (88.07%) other \n",
      "------------------------------\n",
      "------------------------------\n",
      " Loaded test set with:\n",
      "   7721 datapoints\n",
      "   1002 dialogues\n",
      "   871 (11.28%) clarifications\n",
      "   6850 (88.72%) other \n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "sys.argv = \"-empty\".split()\n",
    "params = args()\n",
    "\n",
    "params.path_to_codraw = '.' + params.path_to_codraw\n",
    "params.path_to_annotation = '.' + params.path_to_annotation\n",
    "params.path_to_preprocessed_imgs = '.' + params.path_to_preprocessed_imgs\n",
    "params.path_to_preprocessed_texts = '.' + params.path_to_preprocessed_texts\n",
    "\n",
    "params.task = 'drawer'\n",
    "datasets_drawer = {split: CodrawData(split, params) for split in SPLITS}\n",
    "\n",
    "params.task = 'teller'\n",
    "datasets_teller = {split: CodrawData(split, params) for split in SPLITS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary functions to make the predictions and compute the two evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X):\n",
    "    predictions = model.predict(X)\n",
    "    probs = model.predict_proba(X)\n",
    "    return predictions, probs\n",
    "    \n",
    "\n",
    "def evaluate(target, predictions, probs_icr):\n",
    "    macro_f1 = metrics.f1_score(target, predictions, average='macro')\n",
    "    avp = metrics.average_precision_score(target, probs_icr)\n",
    "    return {'macro_f1': macro_f1, 'avp': avp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trivial Baseline 1: Utterance features as input\n",
    "\n",
    "We train a Logistic Regression model on simple handcrafted features and use it as the first trivial baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawer\n",
    "\n",
    "For the drawer, we use the length of the teller's utterance and a binary BOW representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vec(utterance, vocab):\n",
    "    indexed_utterance = [vocab[word] if word in vocab else vocab[UNK_TOKEN] for word in utterance]\n",
    "    return [1 if position in indexed_utterance else 0 for position in range(len(vocab)) ]\n",
    "\n",
    "def build_features_drawer(dataset, vocab):\n",
    "    assert dataset.task == 'drawer'\n",
    "    X = []\n",
    "    Y = []\n",
    "    for idx, (dialogue_id, turn) in dataset.datapoints.items():\n",
    "        *_, label = dataset[idx]\n",
    "        game = dataset.games[dialogue_id]\n",
    "        utterance = game.dialogue.teller_turns[turn].split()\n",
    "        # build features: length and binary BOW \n",
    "        X.append([len(utterance)] + build_vec(utterance, vocab))\n",
    "        Y.append(label)\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the vocabulary from the training set. We include only tokens that occur at least two times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_teller_train = Counter()\n",
    "for game in datasets_drawer['train'].games.values():\n",
    "    for turn in game.dialogue.teller_turns:\n",
    "        counter_teller_train.update(turn.split())\n",
    "\n",
    "assert UNK_TOKEN not in counter_teller_train\n",
    "vocab_teller_train = [word for (word, count) in counter_teller_train.items() if count >= MIN_FREQ] + [UNK_TOKEN]\n",
    "word2id = {word: i for i, word in enumerate(vocab_teller_train)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = build_features_drawer(datasets_drawer['train'], word2id)\n",
    "X_val, Y_val = build_features_drawer(datasets_drawer['val'], word2id)\n",
    "X_test, Y_test = build_features_drawer(datasets_drawer['test'], word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/blasota/anaconda3/envs/codraw_pl/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "baseline = LogisticRegression(random_state=0, class_weight='balanced', max_iter=MAX_ITER)\n",
    "baseline.fit(X_train, Y_train)\n",
    "\n",
    "predictions, probs = predict(baseline, X_val)\n",
    "probs_icr = probs[:, ICR_LABEL]\n",
    "results[('drawer', 'features', 'val')]= {**evaluate(Y_val, predictions, probs_icr)}\n",
    "\n",
    "predictions, probs = predict(baseline, X_test)\n",
    "probs_icr = probs[:, ICR_LABEL]\n",
    "results[('drawer', 'features', 'test')]= {**evaluate(Y_test, predictions, probs_icr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:  {'macro_f1': 0.5316873922453322, 'avp': 0.20662440606077712}\n",
      "Test:        {'macro_f1': 0.5188309960070748, 'avp': 0.19581780263105944}\n"
     ]
    }
   ],
   "source": [
    "print('Validation: ', results[('drawer', 'features', 'val')])\n",
    "print('Test:       ', results[('drawer', 'features', 'test')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teller\n",
    "\n",
    "For the teller, we use the length of the drawer's utterance and a flag for predefined content words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features_teller(dataset):\n",
    "    assert dataset.task == 'teller'\n",
    "    X, Y = [], []\n",
    "    for idx, (dialogue_id, turn) in dataset.datapoints.items():\n",
    "        *_, label = dataset[idx]\n",
    "        game = dataset.games[dialogue_id]\n",
    "        utterance = game.dialogue.drawer_turns[turn].split()\n",
    "        has_content = 1 if set(utterance).intersection(SELECTED_TOKENS) else 0\n",
    "        # build features: number of tokens and a flag for content tokens\n",
    "        X.append([len(utterance), has_content])\n",
    "        Y.append(label)\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = build_features_teller(datasets_teller['train'])\n",
    "X_val, Y_val = build_features_teller(datasets_teller['val'])\n",
    "X_test, Y_test = build_features_teller(datasets_teller['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = LogisticRegression(random_state=0, class_weight='balanced', max_iter=MAX_ITER)\n",
    "baseline.fit(X_train, Y_train)\n",
    "\n",
    "predictions, probs = predict(baseline, X_val)\n",
    "probs_icr = probs[:, ICR_LABEL]\n",
    "results[('teller', 'features', 'val')]= {**evaluate(Y_val, predictions, probs_icr)}\n",
    "\n",
    "predictions, probs = predict(baseline, X_test)\n",
    "probs_icr = probs[:, ICR_LABEL]\n",
    "results[('teller', 'features', 'test')]= {**evaluate(Y_test, predictions, probs_icr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:  {'macro_f1': 0.8588035508320644, 'avp': 0.6874505958715378}\n",
      "Test:        {'macro_f1': 0.8552709579197477, 'avp': 0.6874181339274107}\n"
     ]
    }
   ],
   "source": [
    "print('Validation: ', results[('teller', 'features', 'val')])\n",
    "print('Test:       ', results[('teller', 'features', 'test')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trivial Baseline 2: Real embeddings as input\n",
    "\n",
    "We train a Logistic Regression model on the same input embeddings and use it as the second trivial baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_input_vectors(dataset):\n",
    "    X_context = []\n",
    "    X_last_msg = []\n",
    "    X_img = []\n",
    "    Y = []\n",
    "    for idx, (dialogue_id, turn) in dataset.datapoints.items():\n",
    "        _, context, last_msg, img, label = dataset[idx]\n",
    "        # build features\n",
    "        X_last_msg.append(last_msg.tolist())\n",
    "        X_context.append(context.tolist())\n",
    "        X_img.append(img.tolist())\n",
    "        Y.append(label)\n",
    "    full_representation = np.concatenate([np.array(X_context), np.array(X_last_msg), np.array(X_img)], axis=1)\n",
    "    return full_representation, np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all, Y_train = build_input_vectors(datasets_drawer['train'])\n",
    "X_val_all, Y_val = build_input_vectors(datasets_drawer['val'])\n",
    "X_test_all, Y_test = build_input_vectors(datasets_drawer['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         3585     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.30216D+04    |proj g|=  9.49739D+02\n",
      "\n",
      "At iterate   50    f=  3.66305D+04    |proj g|=  2.05091D+03\n",
      "\n",
      "At iterate  100    f=  3.48539D+04    |proj g|=  1.53278D+03\n",
      "\n",
      "At iterate  150    f=  3.40219D+04    |proj g|=  1.20084D+03\n",
      "\n",
      "At iterate  200    f=  3.35152D+04    |proj g|=  7.00009D+02\n",
      "\n",
      "At iterate  250    f=  3.29758D+04    |proj g|=  1.52217D+03\n",
      "\n",
      "At iterate  300    f=  3.26114D+04    |proj g|=  1.12622D+02\n",
      "\n",
      "At iterate  350    f=  3.23680D+04    |proj g|=  3.72251D+02\n",
      "\n",
      "At iterate  400    f=  3.21307D+04    |proj g|=  1.67465D+02\n",
      "\n",
      "At iterate  450    f=  3.19325D+04    |proj g|=  6.98250D+02\n",
      "\n",
      "At iterate  500    f=  3.16736D+04    |proj g|=  1.08437D+02\n",
      "\n",
      "At iterate  550    f=  3.15090D+04    |proj g|=  2.01151D+02\n",
      "\n",
      "At iterate  600    f=  3.13805D+04    |proj g|=  3.41389D+02\n",
      "\n",
      "At iterate  650    f=  3.13033D+04    |proj g|=  5.52142D+01\n",
      "\n",
      "At iterate  700    f=  3.12304D+04    |proj g|=  7.91096D+01\n",
      "\n",
      "At iterate  750    f=  3.11880D+04    |proj g|=  2.88175D+02\n",
      "\n",
      "At iterate  800    f=  3.11647D+04    |proj g|=  2.65114D+02\n",
      "\n",
      "At iterate  850    f=  3.11284D+04    |proj g|=  1.16182D+02\n",
      "\n",
      "At iterate  900    f=  3.11073D+04    |proj g|=  1.21198D+02\n",
      "\n",
      "At iterate  950    f=  3.10949D+04    |proj g|=  6.69201D+01\n",
      "\n",
      "At iterate 1000    f=  3.10792D+04    |proj g|=  1.52155D+01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 3585   1000   1160      1     0     0   1.522D+01   3.108D+04\n",
      "  F =   31079.242440219135     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/blasota/anaconda3/envs/codraw_pl/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.6min finished\n"
     ]
    }
   ],
   "source": [
    "baseline = LogisticRegression(random_state=0, class_weight='balanced', max_iter=MAX_ITER, verbose=1)\n",
    "baseline.fit(X_train_all, Y_train)\n",
    "\n",
    "predictions, probs = predict(baseline, X_val_all)\n",
    "probs_icr = probs[:, ICR_LABEL]\n",
    "results[('drawer', 'representations', 'val')]= {**evaluate(Y_val, predictions, probs_icr)}\n",
    "\n",
    "predictions, probs = predict(baseline, X_test_all)\n",
    "probs_icr = probs[:, ICR_LABEL]\n",
    "results[('drawer', 'representations', 'test')]= {**evaluate(Y_test, predictions, probs_icr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:  {'macro_f1': 0.5878799867882846, 'avp': 0.3247402864118646}\n",
      "Test:        {'macro_f1': 0.5766599983865733, 'avp': 0.28715655344114893}\n"
     ]
    }
   ],
   "source": [
    "print('Validation: ', results[('drawer', 'representations', 'val')])\n",
    "print('Test:       ', results[('drawer', 'representations', 'test')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all, Y_train = build_input_vectors(datasets_teller['train'])\n",
    "X_val_all, Y_val = build_input_vectors(datasets_teller['val'])\n",
    "X_test_all, Y_test = build_input_vectors(datasets_teller['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =         3585     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.30216D+04    |proj g|=  1.61178D+03\n",
      "\n",
      "At iterate   50    f=  3.90926D+03    |proj g|=  4.56677D+01\n",
      "\n",
      "At iterate  100    f=  3.22936D+03    |proj g|=  2.71992D+02\n",
      "\n",
      "At iterate  150    f=  2.72263D+03    |proj g|=  1.11848D+02\n",
      "\n",
      "At iterate  200    f=  2.56824D+03    |proj g|=  1.03741D+01\n",
      "\n",
      "At iterate  250    f=  2.50801D+03    |proj g|=  1.93262D+01\n",
      "\n",
      "At iterate  300    f=  2.49093D+03    |proj g|=  1.12515D+01\n",
      "\n",
      "At iterate  350    f=  2.48535D+03    |proj g|=  1.65014D+01\n",
      "\n",
      "At iterate  400    f=  2.48221D+03    |proj g|=  3.20542D+00\n",
      "\n",
      "At iterate  450    f=  2.48051D+03    |proj g|=  1.43569D+01\n",
      "\n",
      "At iterate  500    f=  2.47924D+03    |proj g|=  9.70877D+00\n",
      "\n",
      "At iterate  550    f=  2.47889D+03    |proj g|=  5.12401D-01\n",
      "\n",
      "At iterate  600    f=  2.47872D+03    |proj g|=  8.72673D-01\n",
      "\n",
      "At iterate  650    f=  2.47867D+03    |proj g|=  4.20024D-01\n",
      "\n",
      "At iterate  700    f=  2.47865D+03    |proj g|=  7.05027D-01\n",
      "\n",
      "At iterate  750    f=  2.47864D+03    |proj g|=  1.97586D-01\n",
      "\n",
      "At iterate  800    f=  2.47863D+03    |proj g|=  1.20381D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      " 3585    833    985      1     0     0   9.988D-02   2.479D+03\n",
      "  F =   2478.6310828900864     \n",
      "\n",
      "CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min finished\n"
     ]
    }
   ],
   "source": [
    "baseline = LogisticRegression(random_state=0, class_weight='balanced', max_iter=MAX_ITER, verbose=1)\n",
    "baseline.fit(X_train_all, Y_train)\n",
    "\n",
    "predictions, probs = predict(baseline, X_val_all)\n",
    "probs_icr = probs[:, ICR_LABEL]\n",
    "results[('teller', 'representations', 'val')] = {**evaluate(Y_val, predictions, probs_icr)}\n",
    "\n",
    "predictions, probs = predict(baseline, X_test_all)\n",
    "probs_icr = probs[:, ICR_LABEL]\n",
    "results[('teller', 'representations', 'test')] = {**evaluate(Y_test, predictions, probs_icr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:  {'macro_f1': 0.9627892439474388, 'avp': 0.9840569088144125}\n",
      "Test:        {'macro_f1': 0.9615280590549525, 'avp': 0.9780352120179174}\n"
     ]
    }
   ],
   "source": [
    "print('Validation: ', results[('teller', 'representations', 'val')])\n",
    "print('Test:       ', results[('teller', 'representations', 'test')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all, Y_train = build_input_vectors(datasets_drawer['train'])\n",
    "X_val_all, Y_val = build_input_vectors(datasets_drawer['val'])\n",
    "X_test_all, Y_test = build_input_vectors(datasets_drawer['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_baseline = DummyClassifier(strategy=\"stratified\", random_state=123)\n",
    "random_baseline.fit(X_train_all, Y_train)\n",
    "\n",
    "predictions, probs = predict(random_baseline, X_val_all)\n",
    "probs_icr = probs[:, ICR_LABEL]\n",
    "results[('drawer', 'random', 'val')]= {**evaluate(Y_val, predictions, probs_icr)}\n",
    "\n",
    "predictions, probs = predict(random_baseline, X_test_all)\n",
    "probs_icr = probs[:, ICR_LABEL]\n",
    "results[('drawer', 'random', 'test')]= {**evaluate(Y_test, predictions, probs_icr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:  {'macro_f1': 0.4890836492828434, 'avp': 0.117471253645027}\n",
      "Test:        {'macro_f1': 0.5036599508928662, 'avp': 0.11357712468378736}\n"
     ]
    }
   ],
   "source": [
    "print('Validation: ', results[('drawer', 'random', 'val')])\n",
    "print('Test:       ', results[('drawer', 'random', 'test')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all, Y_train = build_input_vectors(datasets_teller['train'])\n",
    "X_val_all, Y_val = build_input_vectors(datasets_teller['val'])\n",
    "X_test_all, Y_test = build_input_vectors(datasets_teller['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_baseline = DummyClassifier(strategy=\"stratified\", random_state=123)\n",
    "random_baseline.fit(X_train_all, Y_train)\n",
    "\n",
    "predictions, probs = predict(random_baseline, X_val_all)\n",
    "probs_icr = probs[:, ICR_LABEL]\n",
    "results[('teller', 'random', 'val')]= {**evaluate(Y_val, predictions, probs_icr)}\n",
    "\n",
    "predictions, probs = predict(random_baseline, X_test_all)\n",
    "probs_icr = probs[:, ICR_LABEL]\n",
    "results[('teller', 'random', 'test')]= {**evaluate(Y_test, predictions, probs_icr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:  {'macro_f1': 0.4890836492828434, 'avp': 0.117471253645027}\n",
      "Test:        {'macro_f1': 0.5036599508928662, 'avp': 0.11357712468378736}\n"
     ]
    }
   ],
   "source": [
    "print('Validation: ', results[('teller', 'random', 'val')])\n",
    "print('Test:       ', results[('teller', 'random', 'test')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print table for the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random & val & 0.11747 & 0.48908 & 0.11747 & 0.48908 \\\\ \n",
      "random & test & 0.11358 & 0.50366 & 0.11358 & 0.50366 \\\\ \n",
      "features & val & 0.20662 & 0.53169 & 0.68745 & 0.85880 \\\\ \n",
      "features & test & 0.19582 & 0.51883 & 0.68742 & 0.85527 \\\\ \n",
      "representations & val & 0.32474 & 0.58788 & 0.98406 & 0.96279 \\\\ \n",
      "representations & test & 0.28716 & 0.57666 & 0.97804 & 0.96153 \\\\ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "latex_table = ''\n",
    "\n",
    "for baseline in ('random', 'features', 'representations'):\n",
    "    for split  in ('val', 'test'):\n",
    "        outputs = results[('drawer', baseline, split)]\n",
    "        latex_table += f'{baseline} & {split} & {outputs[\"avp\"]:.5f} & {outputs[\"macro_f1\"]:.5f}'\n",
    "        outputs = results[('teller', baseline, split)]\n",
    "        latex_table += f' & {outputs[\"avp\"]:.5f} & {outputs[\"macro_f1\"]:.5f} \\\\\\\\ \\n'\n",
    "\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained dialogue embeddings\n",
    "\n",
    "Can the last embedding before the peek action predict whether a CR occurred?\n",
    "\n",
    "We have not computed the embedding after the last drawer's message, because it's a context we neved use in training. So we either just use the very last context embedding we have (i.e., after the last teller's message), or we try to get the one at the peek action (but then not all dialogues have a peek action)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version 1**: use the very last embedding we have. Here, we have to exclude rare cases where the only iCR is exactly at the last turn, because then the embedding is no including it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_last_context(dataset):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for idx, game in dataset.games.items(): \n",
    "        if game.dialogue.icr_turns == [game.n_turns - 1]:\n",
    "            # corner cases where the only iCR in the dialogue occurs in the very last turn\n",
    "            # we don't have the representation after the last drawer's message,\n",
    "            # which would be necessary for a correct prediction in these cases\n",
    "            # so we exclude them\n",
    "            # this should be a very rare event\n",
    "            print(f'Excluded game {idx}!')\n",
    "            continue\n",
    "        \n",
    "        contains_cr = 1 if game.dialogue.icr_turns else 0\n",
    "\n",
    "        # the last context in our embeddings is the state of the dialogue in the last turn, after the\n",
    "        # teller's utterances but before the drawer's utterances (position 1)\n",
    "        last_context = dataset.embs.contexts[idx][-1][1]\n",
    "        X.append(last_context)\n",
    "        Y.append(contains_cr)\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded game 6257!\n",
      "Excluded game 7395!\n",
      "Excluded game 3911!\n",
      "Excluded game 5455!\n",
      "Excluded game 3580!\n",
      "Excluded game 7403!\n",
      "Excluded game 4045!\n",
      "Excluded game 2320!\n",
      "Excluded game 623!\n",
      "Excluded game 6554!\n",
      "Excluded game 3574!\n",
      "Excluded game 1652!\n",
      "Excluded game 2326!\n",
      "Excluded game 8806!\n",
      "Excluded game 1700!\n",
      "Excluded game 2930!\n",
      "Excluded game 2276!\n",
      "Excluded game 5066!\n",
      "Excluded game 3868!\n",
      "Excluded game 2658!\n",
      "Excluded game 4638!\n",
      "Excluded game 8168!\n",
      "Excluded game 2059!\n",
      "Excluded game 2779!\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = build_last_context(datasets_drawer['train'])\n",
    "X_val, Y_val = build_last_context(datasets_drawer['val'])\n",
    "X_test, Y_test = build_last_context(datasets_drawer['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:  {'macro_f1': 0.8374564168956692, 'avp': 0.8912296111538317}\n",
      "Test:        {'macro_f1': 0.8408753294137334, 'avp': 0.8908863103864789}\n"
     ]
    }
   ],
   "source": [
    "baseline = LogisticRegression(random_state=0, class_weight='balanced')\n",
    "baseline.fit(X_train, Y_train)\n",
    "\n",
    "predictions, probs = predict(baseline, X_val)\n",
    "probs_icr = probs[:, ICR_LABEL]\n",
    "print('Validation: ', {**evaluate(Y_val, predictions, probs_icr)})\n",
    "\n",
    "predictions, probs = predict(baseline, X_test)\n",
    "probs_icr = probs[:, ICR_LABEL]\n",
    "print('Test:       ', {**evaluate(Y_test, predictions, probs_icr)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version 2 (used in the paper)**: We exclude dialogues that do not contain a peek action, so that we can get the representation containing all utterances until the peek (i.e., the context at position 0 at peek turn, which includes the last drawer's utterance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_at_peek(dataset):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for idx, game in dataset.games.items(): \n",
    "        \n",
    "        # exclude dialogues without peek\n",
    "        if game.peek_turn is None:\n",
    "            continue\n",
    "        \n",
    "        turn = game.peek_turn\n",
    "        \n",
    "        contains_cr = 1 if game.dialogue.icr_turns_before_peek else 0\n",
    "\n",
    "        # the state at position 0 at the peek_turn contains the full dialogue, until the last turn before\n",
    "        # the peek\n",
    "        last_context = dataset.embs.contexts[idx][turn][0]\n",
    "        X.append(last_context)\n",
    "        Y.append(contains_cr)\n",
    "\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = build_context_at_peek(datasets_drawer['train'])\n",
    "X_val, Y_val = build_context_at_peek(datasets_drawer['val'])\n",
    "X_test, Y_test = build_context_at_peek(datasets_drawer['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:  {'macro_f1': 0.860035136384651, 'avp': 0.9156948829431442}\n",
      "Test:        {'macro_f1': 0.8569327484838951, 'avp': 0.9037990527661058}\n"
     ]
    }
   ],
   "source": [
    "baseline = LogisticRegression(random_state=0, class_weight='balanced')\n",
    "baseline.fit(X_train, Y_train)\n",
    "\n",
    "predictions, probs = predict(baseline, X_val)\n",
    "probs_icr = probs[:, ICR_LABEL]\n",
    "print('Validation: ', {**evaluate(Y_val, predictions, probs_icr)})\n",
    "\n",
    "predictions, probs = predict(baseline, X_test)\n",
    "probs_icr = probs[:, ICR_LABEL]\n",
    "print('Test:       ', {**evaluate(Y_test, predictions, probs_icr)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version 3**: we use the very last embedding we have before the peek or, if there is no peek, then the last embedding we have for the dialogue. Similar to version 1, we are not including the very last drawer's utterance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_last_context_mixed(dataset):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for idx, game in dataset.games.items():\n",
    "        \n",
    "        if game.peek_turn is None:\n",
    "            assert game.dialogue.icr_turns == game.dialogue.icr_turns_before_peek\n",
    "        \n",
    "        turn = game.peek_turn - 1 if game.peek_turn is not None else game.n_turns - 1\n",
    "        \n",
    "        if game.dialogue.icr_turns_before_peek == [turn]:\n",
    "            # corner cases where the only iCR in the dialogue before peek occurs in the very last turn\n",
    "            # we don't have the representation after the last drawer's message,\n",
    "            # which would be necessary for a correct prediction in these cases\n",
    "            # so we exclude them\n",
    "            # this should be a rare event\n",
    "            print(f'Excluded game {idx}!')\n",
    "            continue\n",
    "        \n",
    "        contains_cr = 1 if game.dialogue.icr_turns_before_peek else 0\n",
    "\n",
    "        # the last context in our embeddings is the state of the dialogue in the last turn, after the\n",
    "        # teller's utterances but before the drawer's utterances\n",
    "        last_context = dataset.embs.contexts[idx][turn][1]\n",
    "        X.append(last_context)\n",
    "        Y.append(contains_cr)\n",
    "\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded game 782!\n",
      "Excluded game 9367!\n",
      "Excluded game 3220!\n",
      "Excluded game 84!\n",
      "Excluded game 7236!\n",
      "Excluded game 2655!\n",
      "Excluded game 8273!\n",
      "Excluded game 9686!\n",
      "Excluded game 7547!\n",
      "Excluded game 5675!\n",
      "Excluded game 5184!\n",
      "Excluded game 4243!\n",
      "Excluded game 3580!\n",
      "Excluded game 4045!\n",
      "Excluded game 4117!\n",
      "Excluded game 7834!\n",
      "Excluded game 4624!\n",
      "Excluded game 1881!\n",
      "Excluded game 6680!\n",
      "Excluded game 9884!\n",
      "Excluded game 2927!\n",
      "Excluded game 3367!\n",
      "Excluded game 5751!\n",
      "Excluded game 8154!\n",
      "Excluded game 1122!\n",
      "Excluded game 7871!\n",
      "Excluded game 6344!\n",
      "Excluded game 1174!\n",
      "Excluded game 3005!\n",
      "Excluded game 465!\n",
      "Excluded game 3385!\n",
      "Excluded game 5734!\n",
      "Excluded game 5242!\n",
      "Excluded game 2577!\n",
      "Excluded game 2087!\n",
      "Excluded game 8934!\n",
      "Excluded game 6556!\n",
      "Excluded game 8307!\n",
      "Excluded game 7453!\n",
      "Excluded game 8530!\n",
      "Excluded game 7431!\n",
      "Excluded game 7360!\n",
      "Excluded game 737!\n",
      "Excluded game 9174!\n",
      "Excluded game 1406!\n",
      "Excluded game 1404!\n",
      "Excluded game 5804!\n",
      "Excluded game 3601!\n",
      "Excluded game 3026!\n",
      "Excluded game 5516!\n",
      "Excluded game 6897!\n",
      "Excluded game 4234!\n",
      "Excluded game 6316!\n",
      "Excluded game 9847!\n",
      "Excluded game 4021!\n",
      "Excluded game 8658!\n",
      "Excluded game 7028!\n",
      "Excluded game 9328!\n",
      "Excluded game 9698!\n",
      "Excluded game 778!\n",
      "Excluded game 8398!\n",
      "Excluded game 1918!\n",
      "Excluded game 7128!\n",
      "Excluded game 7788!\n",
      "Excluded game 4638!\n",
      "Excluded game 1659!\n",
      "Excluded game 7809!\n",
      "Excluded game 889!\n",
      "Excluded game 5659!\n",
      "Excluded game 4379!\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = build_last_context_mixed(datasets_drawer['train'])\n",
    "X_val, Y_val = build_last_context_mixed(datasets_drawer['val'])\n",
    "X_test, Y_test = build_last_context_mixed(datasets_drawer['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:  {'macro_f1': 0.8683673106113321, 'avp': 0.9190822757528802}\n",
      "Test:        {'macro_f1': 0.8527688899827714, 'avp': 0.9099675442767706}\n"
     ]
    }
   ],
   "source": [
    "baseline = LogisticRegression(random_state=0, class_weight='balanced')\n",
    "baseline.fit(X_train, Y_train)\n",
    "\n",
    "predictions, probs = predict(baseline, X_val)\n",
    "probs_icr = probs[:, ICR_LABEL]\n",
    "print('Validation: ', {**evaluate(Y_val, predictions, probs_icr)})\n",
    "\n",
    "predictions, probs = predict(baseline, X_test)\n",
    "probs_icr = probs[:, ICR_LABEL]\n",
    "print('Test:       ', {**evaluate(Y_test, predictions, probs_icr)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codraw_pl",
   "language": "python",
   "name": "codraw_pl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
